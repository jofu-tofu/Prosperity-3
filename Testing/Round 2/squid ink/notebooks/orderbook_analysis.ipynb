{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Analyze Relationship Between Orderbook Features and Returns\n",
    "\n",
    "Now that we have extracted orderbook features before large return events, let's analyze the relationship between these features and the subsequent returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Separate positive and negative return events\n",
    "positive_returns = pre_event_df[pre_event_df['return_direction'] == 'positive']\n",
    "negative_returns = pre_event_df[pre_event_df['return_direction'] == 'negative']\n",
    "\n",
    "print(f\"Number of large positive return events: {len(positive_returns)}\")\n",
    "print(f\"Number of large negative return events: {len(negative_returns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare orderbook features between positive and negative return events\n",
    "feature_comparison = pd.DataFrame({\n",
    "    'positive_mean': positive_returns.mean(),\n",
    "    'negative_mean': negative_returns.mean(),\n",
    "    'positive_median': positive_returns.median(),\n",
    "    'negative_median': negative_returns.median()\n",
    "})\n",
    "\n",
    "# Calculate the difference between positive and negative events\n",
    "feature_comparison['mean_diff'] = feature_comparison['positive_mean'] - feature_comparison['negative_mean']\n",
    "feature_comparison['median_diff'] = feature_comparison['positive_median'] - feature_comparison['negative_median']\n",
    "\n",
    "# Calculate the percentage difference\n",
    "feature_comparison['mean_diff_pct'] = feature_comparison['mean_diff'] / feature_comparison['negative_mean'] * 100\n",
    "feature_comparison['median_diff_pct'] = feature_comparison['median_diff'] / feature_comparison['negative_median'] * 100\n",
    "\n",
    "# Display the comparison for relevant features\n",
    "relevant_features = [\n",
    "    'spread', 'relative_spread', 'volume_imbalance',\n",
    "    'bid_volume_total', 'ask_volume_total', 'book_depth',\n",
    "    'bid_price_impact', 'ask_price_impact',\n",
    "    'price_range', 'relative_price_range'\n",
    "]\n",
    "\n",
    "feature_comparison.loc[relevant_features, ['mean_diff_pct', 'median_diff_pct']].sort_values('mean_diff_pct', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the distribution of key features for positive vs negative returns\n",
    "key_features = [\n",
    "    'volume_imbalance', 'relative_spread', 'book_depth', 'bid_price_impact', 'ask_price_impact'\n",
    "]\n",
    "\n",
    "for feature in key_features:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot histograms\n",
    "    plt.hist(positive_returns[feature].dropna(), bins=20, alpha=0.5, label='Positive Returns')\n",
    "    plt.hist(negative_returns[feature].dropna(), bins=20, alpha=0.5, label='Negative Returns')\n",
    "    \n",
    "    plt.title(f'Distribution of {feature} Before Large Return Events')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate correlation between orderbook features and return values\n",
    "correlation = pre_event_df[relevant_features + ['return_value']].corr()['return_value'].drop('return_value')\n",
    "\n",
    "# Sort by absolute correlation\n",
    "correlation_sorted = correlation.abs().sort_values(ascending=False)\n",
    "\n",
    "# Display the correlations\n",
    "print(\"Correlation between orderbook features and subsequent returns:\")\n",
    "for feature in correlation_sorted.index:\n",
    "    print(f\"{feature}: {correlation[feature]:.4f}\")\n",
    "\n",
    "# Plot the correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation.sort_values().plot(kind='barh')\n",
    "plt.title('Correlation Between Orderbook Features and Subsequent Returns')\n",
    "plt.xlabel('Correlation')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Scatter plots of the most correlated features vs returns\n",
    "top_features = correlation_sorted.index[:3]  # Top 3 most correlated features\n",
    "\n",
    "for feature in top_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(pre_event_df[feature], pre_event_df['return_value'], alpha=0.6)\n",
    "    plt.title(f'Relationship Between {feature} and Subsequent Returns')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Return Value')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Tests\n",
    "\n",
    "Let's perform statistical tests to determine if there are significant differences in orderbook features before positive vs negative returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import statistical testing libraries\n",
    "from scipy import stats\n",
    "\n",
    "# Perform t-tests for each feature\n",
    "t_test_results = {}\n",
    "\n",
    "for feature in relevant_features:\n",
    "    # Get the data for positive and negative returns\n",
    "    pos_data = positive_returns[feature].dropna()\n",
    "    neg_data = negative_returns[feature].dropna()\n",
    "    \n",
    "    # Perform t-test\n",
    "    t_stat, p_value = stats.ttest_ind(pos_data, neg_data, equal_var=False)\n",
    "    \n",
    "    # Store results\n",
    "    t_test_results[feature] = {\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for easier viewing\n",
    "t_test_df = pd.DataFrame(t_test_results).T\n",
    "t_test_df = t_test_df.sort_values('p_value')\n",
    "\n",
    "# Display results\n",
    "print(\"T-test results for orderbook features (positive vs negative returns):\")\n",
    "t_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Let's use a simple machine learning model to determine feature importance for predicting the direction of large returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import machine learning libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Prepare the data\n",
    "X = pre_event_df[relevant_features].copy()\n",
    "y = (pre_event_df['return_value'] > 0).astype(int)  # 1 for positive returns, 0 for negative\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importances['feature'], feature_importances['importance'])\n",
    "plt.title('Feature Importance for Predicting Return Direction')\n",
    "plt.xlabel('Importance')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "\n",
    "Let's summarize our findings about the relationship between orderbook features and large returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify the most significant features based on t-tests\n",
    "significant_features = t_test_df[t_test_df['significant']].index.tolist()\n",
    "\n",
    "# Identify the most important features based on the random forest model\n",
    "top_importance_features = feature_importances.head(5)['feature'].tolist()\n",
    "\n",
    "# Identify the most correlated features\n",
    "top_correlated_features = correlation_sorted.head(5).index.tolist()\n",
    "\n",
    "print(\"Summary of Findings:\")\n",
    "print(\"\\n1. Statistically significant differences between positive and negative returns:\")\n",
    "for feature in significant_features:\n",
    "    t_stat = t_test_df.loc[feature, 't_statistic']\n",
    "    p_value = t_test_df.loc[feature, 'p_value']\n",
    "    pos_mean = positive_returns[feature].mean()\n",
    "    neg_mean = negative_returns[feature].mean()\n",
    "    print(f\"   - {feature}: t={t_stat:.4f}, p={p_value:.4f}\")\n",
    "    print(f\"     Positive returns mean: {pos_mean:.4f}, Negative returns mean: {neg_mean:.4f}\")\n",
    "\n",
    "print(\"\\n2. Top features by correlation with returns:\")\n",
    "for feature in top_correlated_features:\n",
    "    print(f\"   - {feature}: {correlation[feature]:.4f}\")\n",
    "\n",
    "print(\"\\n3. Top features by importance in predicting return direction:\")\n",
    "for feature in top_importance_features:\n",
    "    importance = feature_importances[feature_importances['feature'] == feature]['importance'].values[0]\n",
    "    print(f\"   - {feature}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\n4. Model performance:\")\n",
    "print(f\"   - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n5. Key insights:\")\n",
    "# These will be filled in based on the actual results from the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Let's save the pre-event orderbook states and analysis results for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = '../data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save pre-event orderbook states\n",
    "pre_event_df.to_csv(os.path.join(output_dir, 'squid_pre_event_orderbook_states.csv'), index=False)\n",
    "print(f\"Pre-event orderbook states saved to {os.path.join(output_dir, 'squid_pre_event_orderbook_states.csv')}\")\n",
    "\n",
    "# Save feature importance results\n",
    "feature_importances.to_csv(os.path.join(output_dir, 'squid_feature_importances.csv'), index=False)\n",
    "print(f\"Feature importances saved to {os.path.join(output_dir, 'squid_feature_importances.csv')}\")\n",
    "\n",
    "# Save t-test results\n",
    "t_test_df.to_csv(os.path.join(output_dir, 'squid_t_test_results.csv'))\n",
    "print(f\"T-test results saved to {os.path.join(output_dir, 'squid_t_test_results.csv')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
