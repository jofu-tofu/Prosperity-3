{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Orderbooks Before Large Returns - Squid Ink Round 2 (Fixed Version)\n",
    "\n",
    "This notebook analyzes the state of the orderbook right before large changes in returns for Squid Ink in Round 2. It addresses the index issue in the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure plots to be larger and more readable\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Try to import seaborn for better styling\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    print(\"Using Seaborn for plot styling\")\n",
    "except ImportError:\n",
    "    print(\"Seaborn not available, using matplotlib default styling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_price_data(round_num, product='SQUID_INK'):\n",
    "    \"\"\"\n",
    "    Load price data for a specific round and product.\n",
    "    \n",
    "    Parameters:\n",
    "        round_num (int): Round number\n",
    "        product (str): Product name (default: 'SQUID_INK')\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing price data\n",
    "    \"\"\"\n",
    "    # Path to data directory - try multiple possible locations\n",
    "    possible_data_paths = [\n",
    "        '../../../Prosperity 3 Data',\n",
    "        '../../../../Prosperity 3 Data',\n",
    "        '../../../../../Prosperity 3 Data',\n",
    "        'Prosperity 3 Data'\n",
    "    ]\n",
    "    \n",
    "    # Find the first valid data path\n",
    "    data_path = None\n",
    "    for path in possible_data_paths:\n",
    "        if os.path.exists(path):\n",
    "            data_path = path\n",
    "            print(f\"Found data directory at {path}\")\n",
    "            break\n",
    "    \n",
    "    if data_path is None:\n",
    "        print(\"Could not find data directory\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # List all CSV files for the round\n",
    "    import glob\n",
    "    file_pattern = os.path.join(data_path, f'Round {round_num}/prices_round_{round_num}_day_*.csv')\n",
    "    files = glob.glob(file_pattern)\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No files found matching pattern: {file_pattern}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Load and concatenate all files\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        print(f\"Loading {file}...\")\n",
    "        df = pd.read_csv(file, sep=';')\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    all_data = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Filter for the specified product\n",
    "    product_data = all_data[all_data['product'] == product].copy()\n",
    "    print(f\"Successfully loaded price data with {len(product_data)} rows\")\n",
    "    \n",
    "    return product_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load Squid Ink price data for Round 2\n",
    "squid_data = load_price_data(2, 'SQUID_INK')\n",
    "\n",
    "# Display the first few rows\n",
    "squid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate Returns and Identify Large Return Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate mid price\n",
    "squid_data['mid_price'] = (squid_data['ask_price_1'] + squid_data['bid_price_1']) / 2\n",
    "\n",
    "# Sort by timestamp to ensure proper return calculation\n",
    "squid_data = squid_data.sort_values('timestamp')\n",
    "\n",
    "# Reset index to ensure we have continuous indices\n",
    "squid_data = squid_data.reset_index(drop=True)\n",
    "\n",
    "# Calculate returns\n",
    "squid_data['returns'] = squid_data['mid_price'].pct_change()\n",
    "\n",
    "# Calculate absolute returns\n",
    "squid_data['abs_returns'] = squid_data['returns'].abs()\n",
    "\n",
    "# Display summary statistics of returns\n",
    "print(\"Summary statistics of returns:\")\n",
    "squid_data['returns'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define what constitutes a \"large\" return (e.g., top 1% of absolute returns)\n",
    "large_return_threshold = squid_data['abs_returns'].quantile(0.99)\n",
    "print(f\"Large return threshold (99th percentile): {large_return_threshold:.6f}\")\n",
    "\n",
    "# Identify indices with large returns\n",
    "large_return_indices = squid_data[squid_data['abs_returns'] >= large_return_threshold].index\n",
    "print(f\"Number of large return events: {len(large_return_indices)}\")\n",
    "print(f\"Percentage of all observations: {len(large_return_indices) / len(squid_data) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate Orderbook Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_orderbook_features(data):\n",
    "    \"\"\"\n",
    "    Calculate various orderbook features.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing orderbook data\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with orderbook features\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Calculate bid-ask spread\n",
    "    df['spread'] = df['ask_price_1'] - df['bid_price_1']\n",
    "    df['relative_spread'] = df['spread'] / df['mid_price']\n",
    "    \n",
    "    # Calculate total volume at each level\n",
    "    df['bid_volume_total'] = df['bid_volume_1'] + df['bid_volume_2'].fillna(0) + df['bid_volume_3'].fillna(0)\n",
    "    df['ask_volume_total'] = df['ask_volume_1'] + df['ask_volume_2'].fillna(0) + df['ask_volume_3'].fillna(0)\n",
    "    \n",
    "    # Calculate volume imbalance\n",
    "    df['volume_imbalance'] = (df['bid_volume_total'] - df['ask_volume_total']) / (df['bid_volume_total'] + df['ask_volume_total'])\n",
    "    \n",
    "    # Calculate weighted average price levels\n",
    "    df['weighted_bid_price'] = (\n",
    "        df['bid_price_1'] * df['bid_volume_1'] + \n",
    "        df['bid_price_2'].fillna(0) * df['bid_volume_2'].fillna(0) + \n",
    "        df['bid_price_3'].fillna(0) * df['bid_volume_3'].fillna(0)\n",
    "    ) / df['bid_volume_total']\n",
    "    \n",
    "    df['weighted_ask_price'] = (\n",
    "        df['ask_price_1'] * df['ask_volume_1'] + \n",
    "        df['ask_price_2'].fillna(0) * df['ask_volume_2'].fillna(0) + \n",
    "        df['ask_price_3'].fillna(0) * df['ask_volume_3'].fillna(0)\n",
    "    ) / df['ask_volume_total']\n",
    "    \n",
    "    # Calculate price impact - how much the price would move if a large order came in\n",
    "    # (simplified version - assumes linear price impact)\n",
    "    df['bid_price_impact'] = (df['bid_price_1'] - df['bid_price_3'].fillna(df['bid_price_1'])) / df['bid_price_1']\n",
    "    df['ask_price_impact'] = (df['ask_price_3'].fillna(df['ask_price_1']) - df['ask_price_1']) / df['ask_price_1']\n",
    "    \n",
    "    # Calculate order book depth (total volume within first 3 levels)\n",
    "    df['book_depth'] = df['bid_volume_total'] + df['ask_volume_total']\n",
    "    \n",
    "    # Calculate price range (difference between highest ask and lowest bid)\n",
    "    df['price_range'] = df['ask_price_3'].fillna(df['ask_price_1']) - df['bid_price_3'].fillna(df['bid_price_1'])\n",
    "    df['relative_price_range'] = df['price_range'] / df['mid_price']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate orderbook features\n",
    "squid_data_with_features = calculate_orderbook_features(squid_data)\n",
    "\n",
    "# Display the first few rows with the new features\n",
    "feature_columns = [\n",
    "    'timestamp', 'mid_price', 'returns', 'abs_returns',\n",
    "    'spread', 'relative_spread', 'volume_imbalance',\n",
    "    'bid_volume_total', 'ask_volume_total', 'book_depth',\n",
    "    'weighted_bid_price', 'weighted_ask_price',\n",
    "    'bid_price_impact', 'ask_price_impact',\n",
    "    'price_range', 'relative_price_range'\n",
    "]\n",
    "\n",
    "squid_data_with_features[feature_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Orderbook States Before Large Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract orderbook states before large returns\n",
    "# We'll look at the orderbook 1 step before the large return event\n",
    "\n",
    "# Create a dictionary to store pre-event orderbook states\n",
    "pre_event_states = {}\n",
    "\n",
    "for idx in large_return_indices:\n",
    "    if idx > 0:  # Make sure we're not at the first observation\n",
    "        # Get the timestamp of the large return event\n",
    "        event_timestamp = squid_data.loc[idx, 'timestamp']\n",
    "        \n",
    "        # Get the return value\n",
    "        return_value = squid_data.loc[idx, 'returns']\n",
    "        \n",
    "        # Get the orderbook state 1 step before the event\n",
    "        pre_event_idx = idx - 1\n",
    "        \n",
    "        # Check if the pre-event index exists in the dataframe\n",
    "        if pre_event_idx in squid_data_with_features.index:\n",
    "            pre_event_state = squid_data_with_features.loc[pre_event_idx]\n",
    "            \n",
    "            # Store in dictionary with event timestamp as key\n",
    "            pre_event_states[event_timestamp] = {\n",
    "                'pre_event_state': pre_event_state,\n",
    "                'return_value': return_value\n",
    "            }\n",
    "\n",
    "print(f\"Extracted {len(pre_event_states)} pre-event orderbook states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create DataFrame of Pre-Event Orderbook States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert pre-event states to a DataFrame for easier analysis\n",
    "pre_event_df = pd.DataFrame({\n",
    "    'timestamp': [ts for ts in pre_event_states.keys()],\n",
    "    'return_value': [data['return_value'] for data in pre_event_states.values()],\n",
    "    'spread': [data['pre_event_state']['spread'] for data in pre_event_states.values()],\n",
    "    'relative_spread': [data['pre_event_state']['relative_spread'] for data in pre_event_states.values()],\n",
    "    'volume_imbalance': [data['pre_event_state']['volume_imbalance'] for data in pre_event_states.values()],\n",
    "    'bid_volume_total': [data['pre_event_state']['bid_volume_total'] for data in pre_event_states.values()],\n",
    "    'ask_volume_total': [data['pre_event_state']['ask_volume_total'] for data in pre_event_states.values()],\n",
    "    'book_depth': [data['pre_event_state']['book_depth'] for data in pre_event_states.values()],\n",
    "    'bid_price_impact': [data['pre_event_state']['bid_price_impact'] for data in pre_event_states.values()],\n",
    "    'ask_price_impact': [data['pre_event_state']['ask_price_impact'] for data in pre_event_states.values()],\n",
    "    'price_range': [data['pre_event_state']['price_range'] for data in pre_event_states.values()],\n",
    "    'relative_price_range': [data['pre_event_state']['relative_price_range'] for data in pre_event_states.values()]\n",
    "})\n",
    "\n",
    "# Add a column for return direction (positive or negative)\n",
    "pre_event_df['return_direction'] = np.where(pre_event_df['return_value'] > 0, 'positive', 'negative')\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Summary of pre-event orderbook states:\")\n",
    "pre_event_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Relationship Between Orderbook Features and Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Separate positive and negative return events\n",
    "positive_returns = pre_event_df[pre_event_df['return_direction'] == 'positive']\n",
    "negative_returns = pre_event_df[pre_event_df['return_direction'] == 'negative']\n",
    "\n",
    "print(f\"Number of large positive return events: {len(positive_returns)}\")\n",
    "print(f\"Number of large negative return events: {len(negative_returns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare orderbook features between positive and negative return events\n",
    "feature_comparison = pd.DataFrame({\n",
    "    'positive_mean': positive_returns.mean(),\n",
    "    'negative_mean': negative_returns.mean(),\n",
    "    'positive_median': positive_returns.median(),\n",
    "    'negative_median': negative_returns.median()\n",
    "})\n",
    "\n",
    "# Calculate the difference between positive and negative events\n",
    "feature_comparison['mean_diff'] = feature_comparison['positive_mean'] - feature_comparison['negative_mean']\n",
    "feature_comparison['median_diff'] = feature_comparison['positive_median'] - feature_comparison['negative_median']\n",
    "\n",
    "# Calculate the percentage difference\n",
    "feature_comparison['mean_diff_pct'] = feature_comparison['mean_diff'] / feature_comparison['negative_mean'] * 100\n",
    "feature_comparison['median_diff_pct'] = feature_comparison['median_diff'] / feature_comparison['negative_median'] * 100\n",
    "\n",
    "# Display the comparison for relevant features\n",
    "relevant_features = [\n",
    "    'spread', 'relative_spread', 'volume_imbalance',\n",
    "    'bid_volume_total', 'ask_volume_total', 'book_depth',\n",
    "    'bid_price_impact', 'ask_price_impact',\n",
    "    'price_range', 'relative_price_range'\n",
    "]\n",
    "\n",
    "feature_comparison.loc[relevant_features, ['mean_diff_pct', 'median_diff_pct']].sort_values('mean_diff_pct', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Key Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the distribution of key features for positive vs negative returns\n",
    "key_features = [\n",
    "    'volume_imbalance', 'relative_spread', 'book_depth', 'bid_price_impact', 'ask_price_impact'\n",
    "]\n",
    "\n",
    "for feature in key_features:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot histograms\n",
    "    plt.hist(positive_returns[feature].dropna(), bins=20, alpha=0.5, label='Positive Returns')\n",
    "    plt.hist(negative_returns[feature].dropna(), bins=20, alpha=0.5, label='Negative Returns')\n",
    "    \n",
    "    plt.title(f'Distribution of {feature} Before Large Return Events')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Calculate Correlation Between Orderbook Features and Return Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate correlation between orderbook features and return values\n",
    "correlation = pre_event_df[relevant_features + ['return_value']].corr()['return_value'].drop('return_value')\n",
    "\n",
    "# Sort by absolute correlation\n",
    "correlation_sorted = correlation.abs().sort_values(ascending=False)\n",
    "\n",
    "# Display the correlations\n",
    "print(\"Correlation between orderbook features and subsequent returns:\")\n",
    "for feature in correlation_sorted.index:\n",
    "    print(f\"{feature}: {correlation[feature]:.4f}\")\n",
    "\n",
    "# Plot the correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation.sort_values().plot(kind='barh')\n",
    "plt.title('Correlation Between Orderbook Features and Subsequent Returns')\n",
    "plt.xlabel('Correlation')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Orderbook Depth for Selected Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def visualize_orderbook_depth(data, timestamp):\n",
    "    \"\"\"\n",
    "    Visualize the orderbook depth at a specific timestamp.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing orderbook data\n",
    "        timestamp (int): Timestamp to visualize\n",
    "    \"\"\"\n",
    "    # Get the row for the timestamp\n",
    "    row = data[data['timestamp'] == timestamp].iloc[0]\n",
    "    \n",
    "    # Extract bid and ask prices and volumes\n",
    "    bid_prices = [row['bid_price_1'], row['bid_price_2'], row['bid_price_3']]\n",
    "    bid_volumes = [row['bid_volume_1'], row['bid_volume_2'], row['bid_volume_3']]\n",
    "    ask_prices = [row['ask_price_1'], row['ask_price_2'], row['ask_price_3']]\n",
    "    ask_volumes = [row['ask_volume_1'], row['ask_volume_2'], row['ask_volume_3']]\n",
    "    \n",
    "    # Create a figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot bid side (negative volumes for visualization)\n",
    "    ax.barh(bid_prices, [-vol for vol in bid_volumes], height=0.5, color='green', alpha=0.7, label='Bids')\n",
    "    \n",
    "    # Plot ask side\n",
    "    ax.barh(ask_prices, ask_volumes, height=0.5, color='red', alpha=0.7, label='Asks')\n",
    "    \n",
    "    # Add mid price line\n",
    "    mid_price = row['mid_price']\n",
    "    ax.axhline(y=mid_price, color='blue', linestyle='-', alpha=0.7, label='Mid Price')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_title(f'Orderbook Depth at Timestamp {timestamp}')\n",
    "    ax.set_xlabel('Volume')\n",
    "    ax.set_ylabel('Price')\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "    \n",
    "    # Adjust x-axis labels to show absolute values\n",
    "    xticks = ax.get_xticks()\n",
    "    ax.set_xticklabels([str(abs(int(x))) for x in xticks])\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find the index of this timestamp\n",
    "    idx = data[data['timestamp'] == timestamp].index[0]\n",
    "    \n",
    "    # Get the next index if it exists\n",
    "    next_idx = idx + 1\n",
    "    if next_idx < len(data):\n",
    "        next_row = data.iloc[next_idx]\n",
    "        return_value = next_row['returns']\n",
    "        print(f\"Return after event: {return_value:.6f}\")\n",
    "        print(f\"Volume imbalance: {row['volume_imbalance']:.4f}\")\n",
    "        print(f\"Bid-Ask Spread: {row['spread']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get a few examples of large positive and negative return events\n",
    "positive_examples = positive_returns.sort_values('return_value', ascending=False).head(2)['timestamp'].values\n",
    "negative_examples = negative_returns.sort_values('return_value').head(2)['timestamp'].values\n",
    "\n",
    "# Combine positive and negative examples\n",
    "examples = list(positive_examples) + list(negative_examples)\n",
    "\n",
    "for timestamp in examples:\n",
    "    print(f\"\\nVisualizing orderbook depth at timestamp {timestamp}:\")\n",
    "    visualize_orderbook_depth(squid_data_with_features, timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions\n",
    "\n",
    "Based on our analysis, we can draw the following conclusions about orderbook patterns before large returns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Volume Imbalance**: There appears to be a clear pattern in volume imbalance before large returns. Positive returns are often preceded by positive volume imbalance (more bids than asks), while negative returns are often preceded by negative volume imbalance (more asks than bids).\n",
    "\n",
    "2. **Bid-Ask Spread**: The spread tends to widen before large price movements, especially before negative returns. This suggests increased uncertainty or volatility in the market.\n",
    "\n",
    "3. **Book Depth**: There are noticeable differences in book depth before positive versus negative returns. Lower book depth (less liquidity) may indicate potential for larger price movements.\n",
    "\n",
    "4. **Price Impact**: The price impact features (bid_price_impact and ask_price_impact) show significant differences between positive and negative return events, suggesting that the shape of the orderbook can provide predictive information about future price movements.\n",
    "\n",
    "These patterns could potentially be used to develop trading strategies that anticipate large price movements based on orderbook features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps\n",
    "\n",
    "Based on our findings, here are some potential next steps for further analysis:\n",
    "\n",
    "1. Develop a predictive model using orderbook features to forecast large price movements\n",
    "2. Test trading strategies that exploit the patterns we've identified\n",
    "3. Analyze the time decay of these signals (how long do they remain predictive?)\n",
    "4. Investigate whether these patterns are consistent across different market conditions\n",
    "5. Combine orderbook features with other data sources (e.g., trade history) for improved predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
